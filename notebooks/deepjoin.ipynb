{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████████████████▌                                                                                                                                                                                                                     | 3917/46521 [00:21<03:56, 180.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATALAKE_DIR, csv_filename)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# read as strings\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)                \u001b[38;5;66;03m# replace NaNs with empty strings\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     table_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.conda/envs/hearts/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/hearts/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.conda/envs/hearts/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/hearts/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.conda/envs/hearts/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "File \u001b[0;32m<frozen codecs>:309\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, errors)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "\n",
    "DATA_DIR = \"data/wiki-join-jaccard/\"\n",
    "DATALAKE_DIR = os.path.join(DATA_DIR, \"datalake\")\n",
    "BENCHMARK_FILE = os.path.join(DATA_DIR, \"benchmark.pkl\")\n",
    "\n",
    "# Step 1: Load benchmark.pkl\n",
    "with open(BENCHMARK_FILE, 'rb') as f:\n",
    "    benchmark_data = pickle.load(f)\n",
    "\n",
    "# Step 2: Read all CSV files into a dictionary\n",
    "# We will map (csv_filename) -> { column_name -> list_of_values }\n",
    "all_tables = {}\n",
    "\n",
    "for csv_filename in tqdm(os.listdir(DATALAKE_DIR)):\n",
    "    if not csv_filename.endswith('.csv'):\n",
    "        continue\n",
    "    path = os.path.join(DATALAKE_DIR, csv_filename)\n",
    "    try:\n",
    "        df = pd.read_csv(path, dtype=str, on_bad_lines='skip')  # read as strings\n",
    "        df = df.fillna('')                # replace NaNs with empty strings\n",
    "        table_dict = {}\n",
    "        for col in df.columns:\n",
    "            col_vals = df[col].unique().tolist()\n",
    "            # Remove duplicates and sanitize\n",
    "            col_vals = [val.strip() for val in col_vals if val.strip()]\n",
    "            table_dict[col] = col_vals\n",
    "        all_tables[csv_filename] = table_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {csv_filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Build the list of columns from all_tables\n",
    "columns = []\n",
    "for csv_file, table_dict in tqdm(all_tables.items()):\n",
    "    table_title = os.path.splitext(csv_file)[0]  # Remove the .csv extension\n",
    "    for col_name, values in table_dict.items():\n",
    "        columns.append({\n",
    "            \"table_title\": table_title,\n",
    "            \"column_name\": col_name,\n",
    "            \"values\": values\n",
    "        })\n",
    "\n",
    "print(f\"Number of columns: {len(columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "def column_to_text(table_title: str, column_name: str, col_vals, max_tokens=128) -> str:\n",
    "    \"\"\"\n",
    "    Creates a text summary for a column.\n",
    "    \n",
    "    The summary includes the column name, number of distinct (non-empty) values,\n",
    "    basic statistics (max, min, average length), and a comma-separated list of\n",
    "    unique values sorted by descending TF-IDF computed locally per column.\n",
    "    \n",
    "    Args:\n",
    "      table_title (str): Title of the table (e.g., derived from the CSV filename).\n",
    "      column_name (str): The name of the column.\n",
    "      col_vals (list): List of cell values (strings).\n",
    "      max_tokens (int): Maximum number of tokens in the final summary.\n",
    "    \n",
    "    Returns:\n",
    "      A truncated text summary string.\n",
    "    \"\"\"\n",
    "    # Filter out empty strings and ensure values are strings\n",
    "    filtered_vals = [str(v).strip() for v in col_vals if str(v).strip() != \"\"]\n",
    "    total_rows = len(filtered_vals)\n",
    "    if total_rows == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Count frequency of each unique value\n",
    "    value_counts = Counter(filtered_vals)\n",
    "    \n",
    "    # Compute local TF-IDF for each unique value\n",
    "    tfidf_scores = {}\n",
    "    for value, count in value_counts.items():\n",
    "        tf = count / total_rows\n",
    "        idf = math.log((total_rows + 1) / (count + 1)) + 1\n",
    "        tfidf_scores[value] = tf * idf\n",
    "    \n",
    "    # Sort values by descending TF-IDF score\n",
    "    sorted_values = sorted(tfidf_scores, key=lambda v: tfidf_scores[v], reverse=True)\n",
    "    \n",
    "    n_distinct = len(sorted_values)\n",
    "    lengths = [len(v) for v in sorted_values]\n",
    "    max_len = max(lengths) if lengths else 0\n",
    "    min_len = min(lengths) if lengths else 0\n",
    "    avg_len = sum(lengths) / len(lengths) if lengths else 0\n",
    "    \n",
    "    col_str = \", \".join(sorted_values)\n",
    "    \n",
    "    summary = (f\"{table_title}. {column_name} contains {n_distinct} values \"\n",
    "               f\"({max_len}, {min_len}, {avg_len:.1f}): {col_str}\")\n",
    "    \n",
    "    # Tokenize and truncate\n",
    "    tokens = nltk.word_tokenize(summary)\n",
    "    truncated_tokens = tokens[:int(max_tokens)]\n",
    "    truncated_sentence = \" \".join(truncated_tokens)\n",
    "    return truncated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each column, compute its text summary.\n",
    "corpus_texts = []\n",
    "id_to_metadata = []  # To map index IDs back to (csv_filename, column_name)\n",
    "for col in tqdm(columns):\n",
    "    text = column_to_text(col[\"table_title\"], col[\"column_name\"], col[\"values\"], max_tokens=128)\n",
    "    corpus_texts.append(text)\n",
    "    # Reconstruct CSV filename from table title (append .csv)\n",
    "    csv_filename = col[\"table_title\"] + \".csv\"\n",
    "    id_to_metadata.append((csv_filename, col[\"column_name\"]))\n",
    "\n",
    "print(f\"Total corpus texts: {len(corpus_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "corpus_embeddings = model.encode(corpus_texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "d = corpus_embeddings.shape[1]\n",
    "# Using a simple FlatL2 index here for max accuracy; for larger datasets you may choose HNSW or IVF-PQ.\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(corpus_embeddings)\n",
    "print(f\"FAISS index built with {index.ntotal} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_inference_to_json(model, index, id_to_metadata, benchmark_data, all_tables,\n",
    "                               n_splits=5, split_frac=0.01, max_k=10, max_tokens=128,\n",
    "                               output_json_file=\"evaluation_results.json\"):\n",
    "    \"\"\"\n",
    "    Evaluate inference using n_splits random subsets of the benchmark.\n",
    "    For each split, sample split_frac fraction of the benchmark queries.\n",
    "    For each query and for k = 1 to max_k, compute precision, recall, F1 and average precision (ap).\n",
    "    Aggregates per-query metrics into system-level metrics and saves both into a JSON file.\n",
    "    \n",
    "    Args:\n",
    "      model: A SentenceTransformer model.\n",
    "      index: A FAISS index built over the corpus embeddings.\n",
    "      id_to_metadata: List mapping index IDs to (csv_filename, column_name).\n",
    "      benchmark_data: dict mapping (csv_filename, column_name) -> list of joinable columns.\n",
    "      all_tables: dict mapping csv_filename to a dict of {column_name: column_values} (used in column_to_text).\n",
    "      n_splits (int): Number of random splits.\n",
    "      split_frac (float): Fraction of benchmark queries to sample in each split.\n",
    "      max_k (int): Maximum k value for computing metrics.\n",
    "      max_tokens (int): Maximum tokens to use in the column_to_text conversion.\n",
    "      output_json_file (str): Path to the JSON file where results will be saved.\n",
    "    \n",
    "    Returns:\n",
    "      results (dict): Dictionary with system-level and per-query metrics.\n",
    "    \"\"\"\n",
    "    benchmark_keys = list(benchmark_data.keys())\n",
    "    num_queries = len(benchmark_keys)\n",
    "    \n",
    "    # Accumulators for system metrics over all evaluated queries\n",
    "    system_precision = np.zeros(max_k)\n",
    "    system_recall    = np.zeros(max_k)\n",
    "    system_ap        = np.zeros(max_k)\n",
    "    f1_lists = [[] for _ in range(max_k)]\n",
    "    \n",
    "    # Dictionary to hold per-query results.\n",
    "    per_query_metrics = {}\n",
    "    total_evaluated_queries = 0\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        random.seed(1000 + split)\n",
    "        sample_size = max(1, int(split_frac * num_queries))\n",
    "        sampled_keys = random.sample(benchmark_keys, sample_size)\n",
    "        \n",
    "        for query_key in tqdm(sampled_keys, desc=f\"Split {split+1}/{n_splits}\"):\n",
    "            csv_filename, col_name = query_key\n",
    "            # Skip if the query column is not available\n",
    "            if csv_filename not in all_tables or col_name not in all_tables[csv_filename]:\n",
    "                continue\n",
    "            \n",
    "            table_title = os.path.splitext(csv_filename)[0]\n",
    "            # Assume column_to_text is defined elsewhere\n",
    "            query_text = column_to_text(table_title, col_name, all_tables[csv_filename][col_name],\n",
    "                                        max_tokens=max_tokens)\n",
    "            query_embedding = model.encode([query_text], convert_to_numpy=True)\n",
    "            distances, indices = index.search(query_embedding, max_k)\n",
    "            \n",
    "            # Retrieved candidate list (as given by the index)\n",
    "            retrieved = [id_to_metadata[i] for i in indices[0]]\n",
    "            \n",
    "            # Ground truth set for this query\n",
    "            gt_set = set(benchmark_data[query_key])\n",
    "            total_relevant = len(gt_set)\n",
    "            \n",
    "            # Initialize per-query metric lists\n",
    "            query_metrics = {\n",
    "                'candidates': retrieved,\n",
    "                'ground_truth': list(gt_set),\n",
    "                'precision': [],\n",
    "                'recall': [],\n",
    "                'f1': [],\n",
    "                'ap': []\n",
    "            }\n",
    "            \n",
    "            for k in range(1, max_k+1):\n",
    "                retrieved_k = retrieved[:k]\n",
    "                # Compute precision and recall\n",
    "                num_relevant = sum(1 for item in retrieved_k if item in gt_set)\n",
    "                precision = num_relevant / k\n",
    "                recall = num_relevant / total_relevant if total_relevant > 0 else 0.0\n",
    "                # F1 score\n",
    "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "                \n",
    "                # Compute average precision (AP) for top-k\n",
    "                ap = 0.0\n",
    "                relevant_count = 0\n",
    "                for i, item in enumerate(retrieved_k, start=1):\n",
    "                    if item in gt_set:\n",
    "                        relevant_count += 1\n",
    "                        ap += relevant_count / i\n",
    "                norm = min(total_relevant, k)\n",
    "                ap = ap / norm if norm > 0 else 0.0\n",
    "                \n",
    "                query_metrics['precision'].append(precision)\n",
    "                query_metrics['recall'].append(recall)\n",
    "                query_metrics['f1'].append(f1)\n",
    "                query_metrics['ap'].append(ap)\n",
    "                \n",
    "                # Accumulate system-level metrics\n",
    "                system_precision[k-1] += precision\n",
    "                system_recall[k-1]    += recall\n",
    "                system_ap[k-1]        += ap\n",
    "                f1_lists[k-1].append(f1)\n",
    "            \n",
    "            # Use a unique key per query that also encodes the split\n",
    "            query_key_str = f\"split_{split}_{csv_filename}::{col_name}\"\n",
    "            per_query_metrics[query_key_str] = query_metrics\n",
    "            total_evaluated_queries += 1\n",
    "\n",
    "    # Average system metrics over all queries\n",
    "    if total_evaluated_queries > 0:\n",
    "        avg_system_precision = (system_precision / total_evaluated_queries).tolist()\n",
    "        avg_system_recall = (system_recall / total_evaluated_queries).tolist()\n",
    "        avg_system_ap = (system_ap / total_evaluated_queries).tolist()\n",
    "        avg_system_f1 = [np.mean(f1_list) if f1_list else 0.0 for f1_list in f1_lists]\n",
    "    else:\n",
    "        avg_system_precision = [0.0] * max_k\n",
    "        avg_system_recall = [0.0] * max_k\n",
    "        avg_system_ap = [0.0] * max_k\n",
    "        avg_system_f1 = [0.0] * max_k\n",
    "    \n",
    "    used_k = list(range(1, max_k+1))\n",
    "    metrics_at_k = {\n",
    "        k: {\n",
    "            'precision': avg_system_precision[k-1],\n",
    "            'recall': avg_system_recall[k-1],\n",
    "            'f1': avg_system_f1[k-1],\n",
    "            'map': avg_system_ap[k-1]\n",
    "        }\n",
    "        for k in used_k\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        'system_metrics': {\n",
    "            'precision': avg_system_precision,\n",
    "            'recall': avg_system_recall,\n",
    "            'f1': avg_system_f1,\n",
    "            'map': avg_system_ap,\n",
    "            'used_k': used_k,\n",
    "            'metrics_at_k': metrics_at_k\n",
    "        },\n",
    "        'per_query_metrics': per_query_metrics\n",
    "    }\n",
    "\n",
    "    # if output path doesn't exist, create it\n",
    "    os.makedirs(os.path.dirname(output_json_file), exist_ok=True)\n",
    "    \n",
    "    # Save the results to a JSON file.\n",
    "    with open(output_json_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {output_json_file}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of top results to retrieve (e.g., top 10)\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "results = evaluate_inference_to_json(model, index, id_to_metadata, \n",
    "                                                benchmark_data, all_tables, n_splits=1, split_frac=1, \n",
    "                                                max_k=10, max_tokens=128, output_json_file=\"output/wiki-join-jaccard/deepjoin/deepjoin_mpnet.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def load_fasttext_model():\n",
    "    \"\"\"Load and return FastText model using gensim downloader (caching it locally).\"\"\"\n",
    "    model_path = 'checkpoints/fasttext/fasttext-wiki-news-subwords-300'\n",
    "    if os.path.exists(model_path):\n",
    "        model = KeyedVectors.load(model_path)\n",
    "    else:\n",
    "        model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        model.save(model_path)\n",
    "    return model\n",
    "\n",
    "fasttext_model = load_fasttext_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "def embed_text_with_fasttext(text, fasttext_model):\n",
    "    \"\"\"\n",
    "    Tokenizes the text and returns the average FastText embedding.\n",
    "    If no tokens are found in the FastText vocabulary, returns a zero vector.\n",
    "    \n",
    "    Args:\n",
    "      text (str): The text summary of the column.\n",
    "      fasttext_model: A pre-loaded FastText model.\n",
    "      \n",
    "    Returns:\n",
    "      np.array: A vector representing the embedding of the text.\n",
    "    \"\"\"\n",
    "    # Tokenize the text (ensure you have downloaded punkt from nltk)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Retrieve embeddings for tokens found in the model's vocabulary.\n",
    "    token_vectors = [fasttext_model[word] for word in tokens if word in fasttext_model]\n",
    "    \n",
    "    if not token_vectors:\n",
    "        # Return a zero vector if no token embeddings are found\n",
    "        return np.zeros(fasttext_model.vector_size)\n",
    "    else:\n",
    "        # Average the token vectors to get a single embedding for the column\n",
    "        return np.mean(token_vectors, axis=0)\n",
    "\n",
    "\n",
    "# Compute embeddings for each column text in corpus_texts\n",
    "column_embeddings = []\n",
    "for text in tqdm(corpus_texts, desc=\"Embedding columns\"):\n",
    "    emb = embed_text_with_fasttext(text, fasttext_model)\n",
    "    column_embeddings.append(emb)\n",
    "\n",
    "print(f\"Computed embeddings for {len(column_embeddings)} columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Assume column_embeddings is a list of numpy arrays (each is a fastText embedding)\n",
    "corpus_embeddings = np.vstack(column_embeddings)  # shape: (num_columns, vector_dim)\n",
    "\n",
    "d = corpus_embeddings.shape[1]  # dimension of embeddings\n",
    "\n",
    "# Create a simple index using L2 distance.\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(corpus_embeddings)\n",
    "print(f\"FAISS index built with {index.ntotal} entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_inference_fasttext_to_json(fasttext_model, index, id_to_metadata, benchmark_data, all_tables,\n",
    "                                        n_splits=5, split_frac=0.01, max_k=10, max_tokens=128,\n",
    "                                        output_json_file=\"evaluation_fasttext_results.json\"):\n",
    "    \"\"\"\n",
    "    Evaluate inference using n_splits random subsets of the benchmark with FastText embeddings.\n",
    "    For each split, a fraction of the benchmark queries are sampled. For each query and for k = 1 to max_k,\n",
    "    precision, recall, F1 and average precision (AP) are computed.\n",
    "    \n",
    "    System-level metrics (averaged over all queries) and per-query metrics are aggregated and saved into a JSON file.\n",
    "    \n",
    "    Args:\n",
    "      fasttext_model: A FastText model (gensim KeyedVectors) loaded previously.\n",
    "      index: A FAISS index built over the corpus embeddings.\n",
    "      id_to_metadata: List mapping index IDs to (csv_filename, column_name).\n",
    "      benchmark_data: dict mapping (csv_filename, column_name) -> list of joinable columns.\n",
    "      all_tables: dict mapping csv_filename to a dict of {column_name: column_values} (used in column_to_text).\n",
    "      n_splits (int): Number of random splits.\n",
    "      split_frac (float): Fraction of benchmark queries to sample in each split.\n",
    "      max_k (int): Maximum k value for computing metrics.\n",
    "      max_tokens (int): Maximum tokens to use in the column_to_text conversion.\n",
    "      output_json_file (str): Path to the JSON file where results will be saved.\n",
    "    \n",
    "    Returns:\n",
    "      results (dict): Dictionary with system-level and per-query metrics.\n",
    "    \"\"\"\n",
    "    benchmark_keys = list(benchmark_data.keys())\n",
    "    num_queries = len(benchmark_keys)\n",
    "    \n",
    "    # Accumulators for system metrics over all evaluated queries.\n",
    "    system_precision = np.zeros(max_k)\n",
    "    system_recall    = np.zeros(max_k)\n",
    "    system_ap        = np.zeros(max_k)\n",
    "    f1_lists = [[] for _ in range(max_k)]\n",
    "    \n",
    "    # Dictionary to hold per-query metrics.\n",
    "    per_query_metrics = {}\n",
    "    total_evaluated_queries = 0\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        # Fix the random seed for reproducibility.\n",
    "        random.seed(1000 + split)\n",
    "        sample_size = max(1, int(split_frac * num_queries))\n",
    "        sampled_keys = random.sample(benchmark_keys, sample_size)\n",
    "        \n",
    "        for query_key in tqdm(sampled_keys, desc=f\"Split {split+1}/{n_splits}\"):\n",
    "            csv_filename, col_name = query_key\n",
    "            # Check if the column exists in our datalake.\n",
    "            if csv_filename not in all_tables or col_name not in all_tables[csv_filename]:\n",
    "                continue\n",
    "            \n",
    "            table_title = os.path.splitext(csv_filename)[0]\n",
    "            # Convert the column to text (assuming column_to_text is defined elsewhere).\n",
    "            query_text = column_to_text(table_title, col_name, all_tables[csv_filename][col_name],\n",
    "                                        max_tokens=max_tokens)\n",
    "            # Use FastText to embed the query text.\n",
    "            query_emb = embed_text_with_fasttext(query_text, fasttext_model)\n",
    "            # FAISS expects a 2D array.\n",
    "            query_embedding = query_emb.reshape(1, -1)\n",
    "            \n",
    "            distances, indices = index.search(query_embedding, max_k)\n",
    "            # Build the list of retrieved candidates.\n",
    "            retrieved = [id_to_metadata[idx] for idx in indices[0]]\n",
    "            \n",
    "            # Ground truth for this query.\n",
    "            gt_set = set(benchmark_data[query_key])\n",
    "            total_relevant = len(gt_set)\n",
    "            \n",
    "            # Dictionary to store metrics for the current query.\n",
    "            query_metrics = {\n",
    "                'candidates': retrieved,\n",
    "                'ground_truth': list(gt_set),\n",
    "                'precision': [],\n",
    "                'recall': [],\n",
    "                'f1': [],\n",
    "                'ap': []\n",
    "            }\n",
    "            \n",
    "            # Compute metrics at each k.\n",
    "            for k in range(1, max_k+1):\n",
    "                retrieved_k = retrieved[:k]\n",
    "                num_relevant = sum(1 for item in retrieved_k if item in gt_set)\n",
    "                precision = num_relevant / k\n",
    "                recall = num_relevant / total_relevant if total_relevant > 0 else 0.0\n",
    "                f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "                \n",
    "                # Compute average precision (AP) for top-k.\n",
    "                ap = 0.0\n",
    "                relevant_count = 0\n",
    "                for i, item in enumerate(retrieved_k, start=1):\n",
    "                    if item in gt_set:\n",
    "                        relevant_count += 1\n",
    "                        ap += relevant_count / i\n",
    "                norm = min(total_relevant, k)\n",
    "                ap = ap / norm if norm > 0 else 0.0\n",
    "                \n",
    "                query_metrics['precision'].append(precision)\n",
    "                query_metrics['recall'].append(recall)\n",
    "                query_metrics['f1'].append(f1)\n",
    "                query_metrics['ap'].append(ap)\n",
    "                \n",
    "                # Accumulate system-level metrics.\n",
    "                system_precision[k-1] += precision\n",
    "                system_recall[k-1]    += recall\n",
    "                system_ap[k-1]        += ap\n",
    "                f1_lists[k-1].append(f1)\n",
    "            \n",
    "            # Create a unique key per query (including the split number).\n",
    "            query_key_str = f\"split_{split}_{csv_filename}::{col_name}\"\n",
    "            per_query_metrics[query_key_str] = query_metrics\n",
    "            total_evaluated_queries += 1\n",
    "    \n",
    "    # Average the system metrics over all evaluated queries.\n",
    "    if total_evaluated_queries > 0:\n",
    "        avg_system_precision = (system_precision / total_evaluated_queries).tolist()\n",
    "        avg_system_recall = (system_recall / total_evaluated_queries).tolist()\n",
    "        avg_system_ap = (system_ap / total_evaluated_queries).tolist()\n",
    "        avg_system_f1 = [np.mean(f1_list) if f1_list else 0.0 for f1_list in f1_lists]\n",
    "    else:\n",
    "        avg_system_precision = [0.0] * max_k\n",
    "        avg_system_recall = [0.0] * max_k\n",
    "        avg_system_ap = [0.0] * max_k\n",
    "        avg_system_f1 = [0.0] * max_k\n",
    "    \n",
    "    used_k = list(range(1, max_k+1))\n",
    "    metrics_at_k = {\n",
    "        k: {\n",
    "            'precision': avg_system_precision[k-1],\n",
    "            'recall': avg_system_recall[k-1],\n",
    "            'f1': avg_system_f1[k-1],\n",
    "            'map': avg_system_ap[k-1]\n",
    "        }\n",
    "        for k in used_k\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        'system_metrics': {\n",
    "            'precision': avg_system_precision,\n",
    "            'recall': avg_system_recall,\n",
    "            'f1': avg_system_f1,\n",
    "            'map': avg_system_ap,\n",
    "            'used_k': used_k,\n",
    "            'metrics_at_k': metrics_at_k\n",
    "        },\n",
    "        'per_query_metrics': per_query_metrics\n",
    "    }\n",
    "    \n",
    "    # Print the aggregated system metrics.\n",
    "    for k in range(1, max_k+1):\n",
    "        print(f\"k = {k}: Precision@k = {avg_system_precision[k-1]:.3f}, \"\n",
    "              f\"Recall@k = {avg_system_recall[k-1]:.3f}, F1@k = {avg_system_f1[k-1]:.3f}, \"\n",
    "              f\"MAP@k = {avg_system_ap[k-1]:.3f}\")\n",
    "        \n",
    "    # if output path doesn't exist, create it\n",
    "    os.makedirs(os.path.dirname(output_json_file), exist_ok=True)\n",
    "    \n",
    "    # Save results to a JSON file.\n",
    "    with open(output_json_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {output_json_file}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_inference_fasttext_to_json(fasttext_model, index, id_to_metadata, benchmark_data, \n",
    "                                              all_tables, n_splits=1, split_frac=1, max_k=10, max_tokens=128, \n",
    "                                              output_json_file=\"output/wiki-join-jaccard/deepjoin/deepjoin_fasttext.json\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
